# Speech-Emotions-Recognition
As humans, we find speech to be the most natural way to express ourselves. This reality motivate many researchers to consider speech signal as a quick and effective process to interact between computer and human. It means the computer should have enough knowledge to identify human voice and speech. Although, there is a significant improvement in speech recognition but still researcher are away from natural interplay between computer and human, since computer is not capable of understanding human emotional state. The recognition of emotional speech aims to recognize the emotional condition of individual utterer by applying his/her voice automatically. Speech emotion recognition is mostly beneficial for applications, which need human-computer interaction such as speech synthesis, customer service, education, forensics and medical analysis [1]. Recognizing of emotional conditions in speech signals are so challenge-able area for
several reason. First issue of all speech emotional methods is selecting the best features, which is powerful enough to distinguish between different emotions. The presence of various language, accent, sentences, speaking style, speakers also add another difficulty because these characteristics directly change most of the extracted features include pitch, energy [2]. Furthermore, it is possible to have a more than one specific emotion at the same in the same speech signal, each emotion correlate with a different part of speech signals. Therefore, defines the boundaries between parts of emotion in very challenging task. The majority of works are concentrated on monolingual emotion recognition, and making a presumption that there are no cultural diversity between utterers. However, the multi-lingual emotion classification process have been considered in some research . Emotions are not easily captured in words.A suitable descriptive system for emotions does not exist as yet.It generates the text from speech and then classify the emotion.So, What we lose in text? Speech, like laughter can be lost in the transcription.so,Instead of using text. We used the speech itself as dataset. The model classify the emotion based on audio features with no text.
In this mini project, we learned to recognize emotions from speech. We used an MLPClassifier for this and made use of the soundfile library to read the sound file, and the librosa library to extract features from it. The model deliverers accuracy of 79%
